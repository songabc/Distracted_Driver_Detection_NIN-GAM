{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "import pandas as pd\n",
    "import data_process as process\n",
    "import time\n",
    "import math\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "from sklearn.model_selection import KFold\n",
    "%matplotlib inline  \n",
    "\n",
    "def plot_gallery(images, titles, n_row=10, n_col=5):\n",
    "    pl.figure(figsize=(2.4 * n_col, 2.4 * n_row))\n",
    "    pl.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(len(images)):\n",
    "        pl.subplot(n_row, n_col, i + 1)\n",
    "        pl.imshow(images[i])\n",
    "        pl.title(titles[i], size=10)\n",
    "        pl.xticks(())\n",
    "        pl.yticks(())\n",
    "        \n",
    "\n",
    "def gen_heat_maps(heats,images,outputs,para):\n",
    "    heat_maps = []\n",
    "    for i in range(len(heats)):\n",
    "        heat = heats[i]\n",
    "        image = images[i]\n",
    "        output = outputs[i]\n",
    "        \n",
    "        classify = np.argmax(output)\n",
    "        cm_jet = cm.get_cmap('jet')\n",
    "        sum_h = heat[:,:,0]*para[0,classify]\n",
    "        \n",
    "        for j in range(1,len(para)):\n",
    "            sum_h += heat[:,:,j]*para[j,classify]\n",
    "            \n",
    "        # resize\n",
    "        im = np.array(Image.fromarray(sum_h).resize((224,224)))\n",
    "        im_nor = im / np.max(im)\n",
    "        \n",
    "        # rgba slice to rgb\n",
    "        im = np.array(cm_jet(np.array(im_nor)))[:,:,:3]\n",
    "        \n",
    "        im[np.where(im_nor < 0.2)] = 0\n",
    "        im = np.uint8(im*95+image*159)\n",
    "        heat_maps.append(np.array(im))\n",
    "    return heat_maps\n",
    "\n",
    "def outputs_to_csv(outputs,filenames,path):\n",
    "    pf = pd.DataFrame(outputs,columns=['c0','c1','c2','c3','c4','c5','c6','c7','c8','c9'])\n",
    "    pf['img'] = pd.Series(filenames)\n",
    "    pf = pf[['img','c0','c1','c2','c3','c4','c5','c6','c7','c8','c9']]\n",
    "    pf.to_csv(path,index=False)\n",
    "    \n",
    "def test_set_predict(path,n):\n",
    "    with tf.Session() as sess:\n",
    "        meta_path = path + '.meta'\n",
    "        saver = tf.train.import_meta_graph(meta_path)\n",
    "        saver.restore(sess,path)\n",
    "        graph = tf.get_default_graph()\n",
    "        \n",
    "        x_ = graph.get_tensor_by_name('input_x:0')\n",
    "        is_train_ = graph.get_tensor_by_name('is_train:0')\n",
    "        outputs_ = graph.get_tensor_by_name('outputs:0')\n",
    "        \n",
    "        # forward\n",
    "        images_arr = []\n",
    "        titles_arr = []\n",
    "        count = 0\n",
    "        for test_batch,filename_batch in process.get_test_images(1):\n",
    "            feed = {\n",
    "                x_:test_batch,\n",
    "                is_train_:False,\n",
    "            }\n",
    "            outputs = sess.run([outputs_],feed_dict=feed)\n",
    "            images_arr.extend([np.uint8((item+0.5)*255) for item in test_batch])\n",
    "            titles_arr.append(process.int_txt_dict[np.argmax(outputs[0])])\n",
    "            count += 1\n",
    "            if count == n:\n",
    "                break\n",
    "        \n",
    "        plot_gallery(images_arr,titles_arr)\n",
    "\n",
    "        \n",
    "def model_validation(path,x,y,csv_path):\n",
    "    \n",
    "    # load model\n",
    "    with tf.Session() as sess:\n",
    "        meta_path = path + '.meta'\n",
    "        saver = tf.train.import_meta_graph(meta_path)\n",
    "        saver.restore(sess,path)\n",
    "        graph = tf.get_default_graph()\n",
    "        \n",
    "        x_ = graph.get_tensor_by_name('input_x:0')\n",
    "        labels_ = graph.get_tensor_by_name('input_y:0')\n",
    "        is_train_ = graph.get_tensor_by_name('is_train:0')\n",
    "        heat_map_ = graph.get_tensor_by_name('cam:0')\n",
    "        outputs_ = graph.get_tensor_by_name('outputs:0')\n",
    "        \n",
    "        fully_para = [v for v in tf.global_variables() if v.name == 'fully_connected/fully_connected/kernel:0'][0]\n",
    "        para = sess.run(fully_para)\n",
    "        # forward\n",
    "        images_arr = []\n",
    "        heat_maps_arr = []\n",
    "        outputs_arr = []\n",
    "        titles_arr = []\n",
    "        for x_batch,y_batch in process.get_batches(x,y,1):\n",
    "            feed = {\n",
    "                x_:x_batch,\n",
    "                labels_:y_batch,\n",
    "                is_train_:False,\n",
    "            }\n",
    "            heat_maps,outputs = sess.run([heat_map_,outputs_],feed_dict=feed)\n",
    "            images_arr.extend([item+0.5 for item in x_batch])\n",
    "            heat_maps_arr.extend(heat_maps)\n",
    "            outputs_arr.extend(outputs)\n",
    "            titles_arr.append(process.int_txt_dict[np.argmax(outputs[0])])\n",
    "            \n",
    "        # print result\n",
    "        all_heat_maps = gen_heat_maps(heat_maps_arr,images_arr,outputs_arr,para)\n",
    "        print(\"All result:\")\n",
    "        plot_gallery(all_heat_maps,titles_arr)\n",
    "        \n",
    "        # get error index\n",
    "        equal = np.equal(np.argmax(outputs_arr,1),np.argmax(y,1))\n",
    "        error_index = np.where(equal == False)\n",
    "        \n",
    "        error_heat_maps = np.array(all_heat_maps)[error_index]\n",
    "        error_titles = np.array(titles_arr)[error_index]\n",
    "#         print(\"error classify:\")\n",
    "#         plot_gallery(error_heat_maps,error_titles)\n",
    "        \n",
    "        test_outputs = []\n",
    "        test_file_names = []\n",
    "        \n",
    "        for test_batch,filename_batch in process.get_test_images(1):\n",
    "            feed = {\n",
    "                x_:test_batch,\n",
    "                is_train_:False\n",
    "            }\n",
    "            test_output = sess.run(outputs_,feed_dict=feed)\n",
    "            test_outputs.extend(test_output)\n",
    "            test_file_names.extend(filename_batch)\n",
    "        outputs_to_csv(outputs,test_file_names,csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Vgg13_gam(object):\n",
    "    def __init__(self,x_shape,class_n):\n",
    "        self.width = x_shape[0]\n",
    "        self.height = x_shape[1]\n",
    "        self.channel = x_shape[2]\n",
    "        self.class_n = class_n\n",
    "        print('use default graph')\n",
    "        tf.reset_default_graph()\n",
    "        self.inputs()\n",
    "        self.graph()\n",
    "     \n",
    "    def inputs(self):\n",
    "        self.x = tf.placeholder(tf.float32,(None,self.width,self.height,self.channel),name='input_x')\n",
    "        self.y = tf.placeholder(tf.float32,(None,self.class_n),name='input_y')\n",
    "        self.learning_rate = tf.placeholder(tf.float32,name='learning_rate')\n",
    "        self.keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "        self.is_train = tf.placeholder(tf.bool,name='is_train')\n",
    "        \n",
    "    def cnn_layers(self,x,filters):\n",
    "        cnn_output = tf.layers.conv2d(x,filters,3,1,padding='same',name='cnn_layer')\n",
    "        relu = tf.maximum(cnn_output,0.1*cnn_output,name='leaky_relu')\n",
    "#         return relu\n",
    "        batch_nor = tf.layers.batch_normalization(relu,training=self.is_train,name='batch_nor')\n",
    "        return batch_nor\n",
    "    \n",
    "    def pooling_layers(self,input):\n",
    "        pool = tf.layers.max_pooling2d(input,2,2,'same')\n",
    "        return pool\n",
    "    \n",
    "    def graph(self):\n",
    "        print('init graph')\n",
    "        with tf.variable_scope('layers1'):\n",
    "            cnn1 = self.cnn_layers(self.x,64)\n",
    "        with tf.variable_scope('layers2'):\n",
    "            cnn2 = self.cnn_layers(cnn1,64)\n",
    "        with tf.variable_scope('pooling1'):\n",
    "            pool1 = self.pooling_layers(cnn2)\n",
    "            \n",
    "        with tf.variable_scope('layers3'):\n",
    "            cnn3 = self.cnn_layers(pool1,128)\n",
    "        with tf.variable_scope('layers4'):\n",
    "            cnn4= self.cnn_layers(cnn3,128)\n",
    "        with tf.variable_scope('pooling2'):\n",
    "            pool2 = self.pooling_layers(cnn4)\n",
    "            \n",
    "        with tf.variable_scope('layers5'):\n",
    "            cnn5 = self.cnn_layers(pool2,256)\n",
    "        with tf.variable_scope('layers6'):\n",
    "            cnn6 = self.cnn_layers(cnn5,256)\n",
    "        with tf.variable_scope('layers7'):\n",
    "            cnn7 = self.cnn_layers(cnn6,256)\n",
    "        with tf.variable_scope('pooling3'):\n",
    "            pool3 = self.pooling_layers(cnn7)\n",
    "            \n",
    "        with tf.variable_scope('layers8'):\n",
    "            cnn8 = self.cnn_layers(pool3,512) \n",
    "        with tf.variable_scope('layers9'):\n",
    "            cnn9 = self.cnn_layers(cnn8,512)\n",
    "        with tf.variable_scope('layer10'):\n",
    "            cnn10 = self.cnn_layers(cnn9,512)\n",
    "        with tf.variable_scope('pooling4'):\n",
    "            pool4 = self.pooling_layers(cnn10)\n",
    "            \n",
    "        with tf.variable_scope('layers11'):\n",
    "            cnn11 = self.cnn_layers(pool4,512)\n",
    "        with tf.variable_scope('layers12'):\n",
    "            cnn12 = self.cnn_layers(cnn11,512)\n",
    "        with tf.variable_scope('layers13'):\n",
    "            cnn13 = self.cnn_layers(cnn12,512)\n",
    "            \n",
    "        with tf.variable_scope('GAP'):\n",
    "            raw_cam = tf.nn.avg_pool(cnn13,[1,14,14,1],[1,14,14,1],'SAME',name='reduce_mean')\n",
    "#             raw_cam = tf.layers.conv2d(cnn13,512,14,14,'same',name='reduce_mean')\n",
    "            cam = tf.reshape(raw_cam,[-1,512],name='reshape')\n",
    "            \n",
    "        with tf.variable_scope('drop_out'):\n",
    "            drop_out = tf.layers.dropout(cam,self.keep_prob,name='')\n",
    "            \n",
    "        with tf.variable_scope('fully_connected'):\n",
    "            logits = tf.layers.dense(drop_out,self.class_n,name='fully_connected')\n",
    "            outputs = tf.sigmoid(logits,name='outputs')\n",
    "            \n",
    "        with tf.variable_scope('error'):\n",
    "            loss = tf.losses.softmax_cross_entropy(logits=logits,onehot_labels=self.y)\n",
    "        with tf.variable_scope('optimizer'):\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(loss)\n",
    "            \n",
    "        with tf.variable_scope('metrics'):\n",
    "#             accuracy = tf.metrics.accuracy(tf.argmax(self.y,axis=1),tf.argmax(outputs,axis=1))[1]\n",
    "            equal = tf.equal(tf.argmax(self.y,axis=1),tf.argmax(outputs,axis=1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(equal,tf.float32))\n",
    "              \n",
    "        self.cam = tf.identity(cnn13,name='cam')\n",
    "        \n",
    "        # temp\n",
    "        self.logits = tf.identity(logits,name='logits')\n",
    "        \n",
    "        self.outputs = tf.identity(outputs,name='outputs')\n",
    "        self.loss = tf.identity(loss,name='loss')\n",
    "        self.accuracy = tf.identity(accuracy,name='accuracy')\n",
    "        \n",
    "    def cv_train(self,X,Y,epoch,batch_size,save_path):\n",
    "        saver = tf.train.Saver()\n",
    "        star_time = time.clock()\n",
    "        step = 0\n",
    "        with tf.Session() as sess:\n",
    "            print('init variables')\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "            writer = tf.summary.FileWriter('./graph/')\n",
    "            writer.add_graph(sess.graph)\n",
    "            step = 0\n",
    "            start_time = time.clock()\n",
    "            current_time = start_time\n",
    "            print('training...')\n",
    "            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            for i in range(epoch):\n",
    "                kf = KFold(n_splits=10,shuffle=True)\n",
    "                cv_n = 0\n",
    "                for train_idx,valid_idx in kf.split(X):\n",
    "                    cv_n += 1\n",
    "                    train_x,train_y = X[train_idx],Y[train_idx]\n",
    "                    valid_x,valid_y = X[valid_idx],Y[valid_idx]\n",
    "                    \n",
    "                    train_accuracy = []\n",
    "                    train_loss = []\n",
    "                    for x_batch,y_batch in process.get_batches(train_x,train_y,batch_size):\n",
    "                        step += 1\n",
    "                        feed = {\n",
    "                            self.x:x_batch,\n",
    "                            self.y:y_batch,\n",
    "                            self.is_train:True,\n",
    "                            self.learning_rate:0.001,\n",
    "                            self.keep_prob:1\n",
    "                        }\n",
    "                        _,__,accuracy,loss = sess.run([self.optimizer,extra_update_ops,self.accuracy,self.loss],feed_dict=feed)\n",
    "#                         print(\"Epoch {},cross validation step {}, batch step {}, step accuracy {:.3F}, step loss {:.6F}\".format(i+1,cv_n,step,accuracy,loss))\n",
    "                        train_accuracy.append(accuracy)\n",
    "                        train_loss.append(loss)\n",
    "                        \n",
    "                        if step % 100 == 0:\n",
    "                            train_time = time.clock() - current_time\n",
    "                            current_time = time.clock()\n",
    "                            valid_accuracy = []\n",
    "                            valid_loss = []\n",
    "                            for x_batch,y_batch in process.get_batches(valid_x,valid_y,batch_size):\n",
    "                                feed = {\n",
    "                                    self.x:x_batch,\n",
    "                                    self.y:y_batch,\n",
    "                                    self.is_train:False,\n",
    "                                    self.keep_prob:0.5\n",
    "                                }\n",
    "                                accuracy,loss,cross = sess.run([self.accuracy,self.loss,self.logits],feed_dict=feed)\n",
    "                                valid_accuracy.append(accuracy)\n",
    "                                valid_loss.append(loss)\n",
    "                            valid_time = time.clock() - current_time\n",
    "                            current_time = time.clock()\n",
    "                            total_time = current_time - start_time\n",
    "                            train_avg_acc = np.mean(train_accuracy)\n",
    "                            train_accuracy = []\n",
    "                            train_avg_loss = np.mean(train_loss)\n",
    "                            train_loss = []\n",
    "                            valid_avg_acc = np.mean(valid_accuracy)\n",
    "                            valid_accuracy = []\n",
    "                            valid_avg_loss = np.mean(valid_loss)\n",
    "                            valid_loss = []\n",
    "                            print(\"Epoch:{},cv:{},trainAcc:{:.4F},validAcc:{:.4F},trainLoss:{:.6F},validLoss:{:.6F},trainTime:{:.0F},validTime:{:.0F},totalTime:{:.0F}\".format(i+1,cv_n,train_avg_acc,valid_avg_acc,train_avg_loss,valid_avg_loss,train_time,valid_time,total_time))\n",
    "        \n",
    "            saver.save(sess,save_path)\n",
    "            print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Nin(object):\n",
    "    def __init__(self,x_shape,class_n):\n",
    "        self.width = x_shape[0]\n",
    "        self.height = x_shape[1]\n",
    "        self.channel = x_shape[2]\n",
    "        self.class_n = class_n\n",
    "        print('use default graph')\n",
    "        tf.reset_default_graph()\n",
    "        self.inputs()\n",
    "        self.graph()\n",
    "     \n",
    "    def inputs(self):\n",
    "        self.x = tf.placeholder(tf.float32,(None,self.width,self.height,self.channel),name='input_x')\n",
    "        self.y = tf.placeholder(tf.float32,(None,self.class_n),name='input_y')\n",
    "        self.learning_rate = tf.placeholder(tf.float32,name='learning_rate')\n",
    "        self.keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "        self.is_train = tf.placeholder(tf.bool,name='is_train')\n",
    "        \n",
    "#     def cnn_layers(self,x,filters):\n",
    "#         cnn_output = tf.layers.conv2d(x,filters,3,1,padding='same',name='cnn_layer')\n",
    "#         relu = tf.maximum(cnn_output,0.1*cnn_output,name='leaky_relu')\n",
    "# #         return relu\n",
    "#         batch_nor = tf.layers.batch_normalization(relu,training=self.is_train,name='batch_nor')\n",
    "#         return batch_nor\n",
    "    \n",
    "#     def pooling_layers(self,input):\n",
    "#         pool = tf.layers.max_pooling2d(input,2,2,'same')\n",
    "#         return pool\n",
    "    \n",
    "    def graph(self):\n",
    "        print('init graph')\n",
    "#         with tf.variable_scope('layers1'):\n",
    "#             cnn1 = self.cnn_layers(self.x,64)\n",
    "#         with tf.variable_scope('layers2'):\n",
    "#             cnn2 = self.cnn_layers(cnn1,64)\n",
    "#         with tf.variable_scope('pooling1'):\n",
    "#             pool1 = self.pooling_layers(cnn2)\n",
    "            \n",
    "        with tf.variable_scope('MLpconv1'):\n",
    "            cnn1 = tf.layers.conv2d(self.x,128,3,1,padding='same',activation=tf.nn.relu,name='cnn_layer')\n",
    "            batch_nor1 = tf.layers.batch_normalization(cnn1,training=self.is_train,name='batch_nor')\n",
    "            mlp1 = tf.layers.conv2d(batch_nor1,128,1,1,padding='same',activation=tf.nn.relu,name='mlp1')\n",
    "            mlp2 = tf.layers.conv2d(mlp1,128,1,1,padding='same',activation=tf.nn.relu,name='mlp2')\n",
    "            pool1 = tf.layers.max_pooling2d(mlp2,2,2,'same')\n",
    "            \n",
    "        with tf.variable_scope('MLpconv2'):\n",
    "            cnn2 = tf.layers.conv2d(pool1,256,3,1,padding='same',activation=tf.nn.relu,name='cnn_layers')\n",
    "            batch_nor2 = tf.layers.batch_normalization(cnn2,training=self.is_train,name='batch_nor')\n",
    "            mlp3 = tf.layers.conv2d(batch_nor2,256,1,1,padding='same',activation=tf.nn.relu,name='mlp1')\n",
    "            mlp4 = tf.layers.conv2d(mlp3,256,1,1,padding='same',activation=tf.nn.relu,name='mlp2')\n",
    "            pool2 = tf.layers.max_pooling2d(mlp4,2,2,'same')\n",
    "            \n",
    "        with tf.variable_scope('MLpconv3'):\n",
    "            cnn3 = tf.layers.conv2d(pool2,512,3,1,padding='same',activation=tf.nn.relu,name='cnn_layers')\n",
    "            batch_nor3 = tf.layers.batch_normalization(cnn3,training=self.is_train,name='batch_nor')\n",
    "            mlp5 = tf.layers.conv2d(batch_nor3,512,1,1,padding='same',activation=tf.nn.relu,name='mlp1')\n",
    "            mlp6 = tf.layers.conv2d(mlp5,512,1,1,padding='same',activation=tf.nn.relu,name='mlp2')\n",
    "            pool3 = tf.layers.max_pooling2d(mlp6,2,2,'same')\n",
    "            \n",
    "        with tf.variable_scope('MLpconv4'):\n",
    "            cnn4 = tf.layers.conv2d(pool3,512,3,1,padding='same',activation=tf.nn.relu,name='cnn_layers1')\n",
    "            cnn5 = tf.layers.conv2d(cnn4,512,3,1,padding='same',activation=tf.nn.relu,name='cnn_layers2')\n",
    "            cnn6 = tf.layers.conv2d(cnn5,512,3,1,padding='same',activation=tf.nn.relu,name='cnn_layers3')\n",
    "            \n",
    "            \n",
    "        with tf.variable_scope('GAP'):\n",
    "            raw_cam = tf.nn.avg_pool(cnn6,[1,28,28,1],[1,28,28,1],'SAME',name='reduce_mean')\n",
    "            cam = tf.reshape(raw_cam,[-1,512],name='reshape')\n",
    "            \n",
    "        with tf.variable_scope('drop_out'):\n",
    "            drop_out = tf.layers.dropout(cam,self.keep_prob,name='')\n",
    "            \n",
    "        with tf.variable_scope('fully_connected'):\n",
    "            logits = tf.layers.dense(drop_out,self.class_n,name='fully_connected')\n",
    "            outputs = tf.sigmoid(logits,name='outputs')\n",
    "            \n",
    "        with tf.variable_scope('error'):\n",
    "            loss = tf.losses.softmax_cross_entropy(logits=logits,onehot_labels=self.y)\n",
    "        with tf.variable_scope('optimizer'):\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(loss)\n",
    "            \n",
    "        with tf.variable_scope('metrics'):\n",
    "#             accuracy = tf.metrics.accuracy(tf.argmax(self.y,axis=1),tf.argmax(outputs,axis=1))[1]\n",
    "            equal = tf.equal(tf.argmax(self.y,axis=1),tf.argmax(outputs,axis=1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(equal,tf.float32))\n",
    "              \n",
    "        self.cam = tf.identity(cnn6,name='cam')\n",
    "        \n",
    "        # temp\n",
    "        self.logits = tf.identity(logits,name='logits')\n",
    "        \n",
    "        self.outputs = tf.identity(outputs,name='outputs')\n",
    "        self.loss = tf.identity(loss,name='loss')\n",
    "        self.accuracy = tf.identity(accuracy,name='accuracy')\n",
    "        \n",
    "    def cv_train(self,X,Y,epoch,batch_size,save_path):\n",
    "        saver = tf.train.Saver()\n",
    "        star_time = time.clock()\n",
    "        step = 0\n",
    "        with tf.Session() as sess:\n",
    "            print('init variables')\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "            writer = tf.summary.FileWriter('./graph/')\n",
    "            writer.add_graph(sess.graph)\n",
    "            step = 0\n",
    "            start_time = time.clock()\n",
    "            current_time = start_time\n",
    "            print('training...')\n",
    "            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            for i in range(epoch):\n",
    "                kf = KFold(n_splits=10,shuffle=True)\n",
    "                cv_n = 0\n",
    "                for train_idx,valid_idx in kf.split(X):\n",
    "                    cv_n += 1\n",
    "                    train_x,train_y = X[train_idx],Y[train_idx]\n",
    "                    valid_x,valid_y = X[valid_idx],Y[valid_idx]\n",
    "                    \n",
    "                    train_accuracy = []\n",
    "                    train_loss = []\n",
    "                    for x_batch,y_batch in process.get_batches(train_x,train_y,batch_size):\n",
    "                        step += 1\n",
    "                        feed = {\n",
    "                            self.x:x_batch,\n",
    "                            self.y:y_batch,\n",
    "                            self.is_train:True,\n",
    "                            self.learning_rate:0.0001,\n",
    "                            self.keep_prob:0.5\n",
    "                        }\n",
    "                        _,__,accuracy,loss = sess.run([self.optimizer,extra_update_ops,self.accuracy,self.loss],feed_dict=feed)\n",
    "#                         print(\"Epoch {},cross validation step {}, batch step {}, step accuracy {:.3F}, step loss {:.6F}\".format(i+1,cv_n,step,accuracy,loss))\n",
    "                        train_accuracy.append(accuracy)\n",
    "                        train_loss.append(loss)\n",
    "                        if step % 100 == 0:\n",
    "                            train_time = time.clock() - current_time\n",
    "                            current_time = time.clock()\n",
    "                            valid_accuracy = []\n",
    "                            valid_loss = []\n",
    "                            for x_batch,y_batch in process.get_batches(valid_x,valid_y,batch_size):\n",
    "                                feed = {\n",
    "                                    self.x:x_batch,\n",
    "                                    self.y:y_batch,\n",
    "                                    self.is_train:False,\n",
    "                                    self.keep_prob:1\n",
    "                                }\n",
    "                                accuracy,loss,cross = sess.run([self.accuracy,self.loss,self.logits],feed_dict=feed)\n",
    "                                valid_accuracy.append(accuracy)\n",
    "                                valid_loss.append(loss)\n",
    "                            valid_time = time.clock() - current_time\n",
    "                            current_time = time.clock()\n",
    "                            total_time = current_time - start_time\n",
    "                            train_avg_acc = np.mean(train_accuracy)\n",
    "                            train_accuracy = []\n",
    "                            train_avg_loss = np.mean(train_loss)\n",
    "                            train_loss = []\n",
    "                            valid_avg_acc = np.mean(valid_accuracy)\n",
    "                            valid_accuracy = []\n",
    "                            valid_avg_loss = np.mean(valid_loss)\n",
    "                            valid_loss = []\n",
    "                            print(\"Epoch:{},cv:{},trainAcc:{:.4F},validAcc:{:.4F},trainLoss:{:.6F},validLoss:{:.6F},trainTime:{:.0F},validTime:{:.0F},totalTime:{:.0F}\".format(i+1,cv_n,train_avg_acc,valid_avg_acc,train_avg_loss,valid_avg_loss,train_time,valid_time,total_time))\n",
    "        \n",
    "                saver.save(sess,save_path,global_step=i+1)\n",
    "            print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Deep_nin(object):\n",
    "    def __init__(self,x_shape,class_n):\n",
    "        self.width = x_shape[0]\n",
    "        self.height = x_shape[1]\n",
    "        self.channel = x_shape[2]\n",
    "        self.class_n = class_n\n",
    "        print('use default graph')\n",
    "        tf.reset_default_graph()\n",
    "        self.inputs()\n",
    "        self.graph()\n",
    "     \n",
    "    def inputs(self):\n",
    "        self.x = tf.placeholder(tf.float32,(None,self.width,self.height,self.channel),name='input_x')\n",
    "        self.y = tf.placeholder(tf.float32,(None,self.class_n),name='input_y')\n",
    "        self.learning_rate = tf.placeholder(tf.float32,name='learning_rate')\n",
    "        self.keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "        self.is_train = tf.placeholder(tf.bool,name='is_train')\n",
    "        \n",
    "#     def cnn_layers(self,x,filters):\n",
    "#         cnn_output = tf.layers.conv2d(x,filters,3,1,padding='same',name='cnn_layer')\n",
    "#         relu = tf.maximum(cnn_output,0.1*cnn_output,name='leaky_relu')\n",
    "# #         return relu\n",
    "#         batch_nor = tf.layers.batch_normalization(relu,training=self.is_train,name='batch_nor')\n",
    "#         return batch_nor\n",
    "    \n",
    "#     def pooling_layers(self,input):\n",
    "#         pool = tf.layers.max_pooling2d(input,2,2,'same')\n",
    "#         return pool\n",
    "    \n",
    "    def graph(self):\n",
    "        print('init graph')\n",
    "#         with tf.variable_scope('layers1'):\n",
    "#             cnn1 = self.cnn_layers(self.x,64)\n",
    "#         with tf.variable_scope('layers2'):\n",
    "#             cnn2 = self.cnn_layers(cnn1,64)\n",
    "#         with tf.variable_scope('pooling1'):\n",
    "#             pool1 = self.pooling_layers(cnn2)\n",
    "            \n",
    "        with tf.variable_scope('MLpconv1'):\n",
    "            cnn1 = tf.layers.conv2d(self.x,128,3,1,padding='same',activation=tf.nn.relu,name='cnn_layer1')\n",
    "            batch_nor1 = tf.layers.batch_normalization(cnn1,training=self.is_train,name='batch_nor1')\n",
    "            cnn1_2 = tf.layers.conv2d(batch_nor1,123,3,1,padding='same',activation=tf.nn.relu,name='cnn_layer2')\n",
    "            batch_nor1_2 = tf.layers.batch_normalization(cnn1_2,training=self.is_train,name='batch_nor2')\n",
    "            mlp1 = tf.layers.conv2d(batch_nor1_2,128,1,1,padding='same',activation=tf.nn.relu,name='mlp1')\n",
    "            mlp1_2 = tf.layers.conv2d(mlp1,128,1,1,padding='same',activation=tf.nn.relu,name='mlp2')\n",
    "            mlp1_3 = tf.layers.conv2d(mlp1_2,128,1,1,padding='same',activation=tf.nn.relu,name='mlp3')\n",
    "            pool1 = tf.layers.max_pooling2d(mlp1_3,2,2,'same')\n",
    "            \n",
    "        with tf.variable_scope('MLpconv2'):\n",
    "            cnn2 = tf.layers.conv2d(pool1,256,3,1,padding='same',activation=tf.nn.relu,name='cnn_layers1')\n",
    "            batch_nor2 = tf.layers.batch_normalization(cnn2,training=self.is_train,name='batch_nor1')\n",
    "            cnn2_2 = tf.layers.conv2d(batch_nor2,256,3,1,padding='same',activation=tf.nn.relu,name='cnn_layers2')\n",
    "            batch_nor2_2 = tf.layers.batch_normalization(cnn2_2,training=self.is_train,name='batch_nor2')\n",
    "            mlp2 = tf.layers.conv2d(batch_nor2_2,256,1,1,padding='same',activation=tf.nn.relu,name='mlp1')\n",
    "            mlp2_2 = tf.layers.conv2d(mlp2,256,1,1,padding='same',activation=tf.nn.relu,name='mlp2')\n",
    "            mlp2_3 = tf.layers.conv2d(mlp2_2,256,1,1,padding='same',activation=tf.nn.relu,name='mlp3')\n",
    "            pool2 = tf.layers.max_pooling2d(mlp2_3,2,2,'same')\n",
    "            \n",
    "        with tf.variable_scope('MLpconv3'):\n",
    "            cnn3 = tf.layers.conv2d(pool2,512,3,1,padding='same',activation=tf.nn.relu,name='cnn_layers1')\n",
    "            batch_nor3 = tf.layers.batch_normalization(cnn3,training=self.is_train,name='batch_nor1')\n",
    "            cnn3_2 = tf.layers.conv2d(batch_nor3,512,3,1,padding='same',activation=tf.nn.relu,name='cnn_layers2')\n",
    "            batch_nor3_2 = tf.layers.batch_normalization(cnn3_2,training=self.is_train,name='batch_nor2')\n",
    "            mlp3 = tf.layers.conv2d(batch_nor3_2,512,1,1,padding='same',activation=tf.nn.relu,name='mlp1')\n",
    "            mlp3_2 = tf.layers.conv2d(mlp3,512,1,1,padding='same',activation=tf.nn.relu,name='mlp2')\n",
    "            mlp3_3 = tf.layers.conv2d(mlp3_2,512,1,1,padding='same',activation=tf.nn.relu,name='mlp3')\n",
    "            pool3 = tf.layers.max_pooling2d(mlp3_3,2,2,'same')\n",
    "            \n",
    "        with tf.variable_scope('MLpconv4'):\n",
    "            cnn4 = tf.layers.conv2d(pool3,512,3,1,padding='same',activation=tf.nn.relu,name='cnn_layers1')\n",
    "            batch_nor4 = tf.layers.batch_normalization(cnn4,training=self.is_train,name='batch_nor1')\n",
    "            cnn5 = tf.layers.conv2d(batch_nor4,512,3,1,padding='same',activation=tf.nn.relu,name='cnn_layers2')\n",
    "            batch_nor5 = tf.layers.batch_normalization(cnn5,training=self.is_train,name='batch_nor2')\n",
    "            cnn6 = tf.layers.conv2d(batch_nor5,512,3,1,padding='same',activation=tf.nn.relu,name='cnn_layers3')\n",
    "            \n",
    "            \n",
    "        with tf.variable_scope('GAP'):\n",
    "            raw_cam = tf.nn.avg_pool(cnn6,[1,28,28,1],[1,28,28,1],'SAME',name='reduce_mean')\n",
    "            cam = tf.reshape(raw_cam,[-1,512],name='reshape')\n",
    "            \n",
    "        with tf.variable_scope('drop_out'):\n",
    "            drop_out = tf.layers.dropout(cam,self.keep_prob,name='')\n",
    "            \n",
    "        with tf.variable_scope('fully_connected'):\n",
    "            logits = tf.layers.dense(drop_out,self.class_n,name='fully_connected')\n",
    "            outputs = tf.sigmoid(logits,name='outputs')\n",
    "            \n",
    "        with tf.variable_scope('error'):\n",
    "            loss = tf.losses.softmax_cross_entropy(logits=logits,onehot_labels=self.y)\n",
    "        with tf.variable_scope('optimizer'):\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(loss)\n",
    "            \n",
    "        with tf.variable_scope('metrics'):\n",
    "#             accuracy = tf.metrics.accuracy(tf.argmax(self.y,axis=1),tf.argmax(outputs,axis=1))[1]\n",
    "            equal = tf.equal(tf.argmax(self.y,axis=1),tf.argmax(outputs,axis=1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(equal,tf.float32))\n",
    "              \n",
    "        self.cam = tf.identity(cnn6,name='cam')\n",
    "        \n",
    "        # temp\n",
    "        self.logits = tf.identity(logits,name='logits')\n",
    "        \n",
    "        self.outputs = tf.identity(outputs,name='outputs')\n",
    "        self.loss = tf.identity(loss,name='loss')\n",
    "        self.accuracy = tf.identity(accuracy,name='accuracy')\n",
    "        \n",
    "    def cv_train(self,X,Y,epoch,batch_size,save_path):\n",
    "        saver = tf.train.Saver()\n",
    "        star_time = time.clock()\n",
    "        step = 0\n",
    "        with tf.Session() as sess:\n",
    "            print('init variables')\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "            writer = tf.summary.FileWriter('./graph/')\n",
    "            writer.add_graph(sess.graph)\n",
    "            step = 0\n",
    "            start_time = time.clock()\n",
    "            current_time = start_time\n",
    "            print('training...')\n",
    "            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            for i in range(epoch):\n",
    "                kf = KFold(n_splits=10,shuffle=True)\n",
    "                cv_n = 0\n",
    "                for train_idx,valid_idx in kf.split(X):\n",
    "                    cv_n += 1\n",
    "                    train_x,train_y = X[train_idx],Y[train_idx]\n",
    "                    valid_x,valid_y = X[valid_idx],Y[valid_idx]\n",
    "                    \n",
    "                    train_accuracy = []\n",
    "                    train_loss = []\n",
    "                    for x_batch,y_batch in process.get_batches(train_x,train_y,batch_size):\n",
    "                        step += 1\n",
    "                        feed = {\n",
    "                            self.x:x_batch,\n",
    "                            self.y:y_batch,\n",
    "                            self.is_train:True,\n",
    "                            self.learning_rate:0.0001,\n",
    "                            self.keep_prob:0.5\n",
    "                        }\n",
    "                        _,__,accuracy,loss = sess.run([self.optimizer,extra_update_ops,self.accuracy,self.loss],feed_dict=feed)\n",
    "#                         print(\"Epoch {},cross validation step {}, batch step {}, step accuracy {:.3F}, step loss {:.6F}\".format(i+1,cv_n,step,accuracy,loss))\n",
    "                        train_accuracy.append(accuracy)\n",
    "                        train_loss.append(loss)\n",
    "                        if step % 100 == 0:\n",
    "                            train_time = time.clock() - current_time\n",
    "                            current_time = time.clock()\n",
    "                            valid_accuracy = []\n",
    "                            valid_loss = []\n",
    "                            for x_batch,y_batch in process.get_batches(valid_x,valid_y,batch_size):\n",
    "                                feed = {\n",
    "                                    self.x:x_batch,\n",
    "                                    self.y:y_batch,\n",
    "                                    self.is_train:False,\n",
    "                                    self.keep_prob:1\n",
    "                                }\n",
    "                                accuracy,loss,cross = sess.run([self.accuracy,self.loss,self.logits],feed_dict=feed)\n",
    "                                valid_accuracy.append(accuracy)\n",
    "                                valid_loss.append(loss)\n",
    "                            valid_time = time.clock() - current_time\n",
    "                            current_time = time.clock()\n",
    "                            total_time = current_time - start_time\n",
    "                            train_avg_acc = np.mean(train_accuracy)\n",
    "                            train_accuracy = []\n",
    "                            train_avg_loss = np.mean(train_loss)\n",
    "                            train_loss = []\n",
    "                            valid_avg_acc = np.mean(valid_accuracy)\n",
    "                            valid_accuracy = []\n",
    "                            valid_avg_loss = np.mean(valid_loss)\n",
    "                            valid_loss = []\n",
    "                            print(\"Epoch:{},cv:{},trainAcc:{:.4F},validAcc:{:.4F},trainLoss:{:.6F},validLoss:{:.6F},trainTime:{:.0F},validTime:{:.0F},totalTime:{:.0F}\".format(i+1,cv_n,train_avg_acc,valid_avg_acc,train_avg_loss,valid_avg_loss,train_time,valid_time,total_time))\n",
    "                saver.save(sess,save_path,global_step=i+1)\n",
    "            print('finish')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
